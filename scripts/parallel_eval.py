"""åˆ†å¸ƒå¼è‡ªå¯¹å¼ˆç³»ç»Ÿ - æ”¯æŒå¹¶è¡Œè¿è¡Œå’Œç»“æœåˆå¹¶

ä½¿ç”¨æ–¹æ³•ï¼š
1. ç‹¬ç«‹è¿è¡Œå¤šä¸ªå°æ‰¹æ¬¡ï¼š
   python scripts/parallel_eval.py --batch-id 1 --total-batches 4 --games-per-batch 50

2. åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœï¼š
   python scripts/parallel_eval.py --merge

è¿™æ ·å¯ä»¥åœ¨å¤šå°æœºå™¨æˆ–å¤šä¸ªè¿›ç¨‹ä¸­å¹¶è¡Œè¿è¡Œï¼Œå¤§å¤§åŠ å¿«è¯„ä¼°é€Ÿåº¦ã€‚
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import argparse
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict
import pandas as pd

from backend.services.winplay_service import SelfPlayEngine, GameResult
from backend.algorithms.classic_ai import GreedyAgent, MinimaxAgent, AlphaBetaAgent
from backend.algorithms.mcts_ai import MCTSAgent
from backend.algorithms.qlearning_ai import QLearningAgent


def get_batch_output_path(batch_id: int) -> Path:
    """è·å–æ‰¹æ¬¡è¾“å‡ºè·¯å¾„"""
    return Path(f"./data/results/self_play/batch_{batch_id}.json")


def get_merge_output_dir() -> Path:
    """è·å–åˆå¹¶è¾“å‡ºç›®å½•"""
    return Path("./data/results/self_play/merged")


def run_batch(batch_id: int, total_batches: int, games_per_batch: int, 
              algorithms: List[str] = None):
    """è¿è¡Œä¸€ä¸ªæ‰¹æ¬¡çš„è‡ªå¯¹å¼ˆ
    
    Args:
        batch_id: æ‰¹æ¬¡ID (1-based)
        total_batches: æ€»æ‰¹æ¬¡æ•°
        games_per_batch: æ¯ä¸ªæ‰¹æ¬¡çš„æ¸¸æˆæ•°
        algorithms: è¦æµ‹è¯•çš„ç®—æ³•åˆ—è¡¨
    """
    print("=" * 70)
    print(f" æ‰¹æ¬¡ {batch_id}/{total_batches} - è‡ªå¯¹å¼ˆè¯„ä¼°")
    print("=" * 70)
    print(f"æ¯ä¸ªé…å¯¹: {games_per_batch} å±€")
    
    # åˆå§‹åŒ–å¼•æ“
    engine = SelfPlayEngine(board_size=15, use_wandb=False)
    
    # æ³¨å†ŒAIç®—æ³•
    if algorithms is None:
        algorithms = ["Greedy", "Minimax-D2", "AlphaBeta-D2", "MCTS-300", "DQN"]
    
    print("\næ³¨å†ŒAIç®—æ³•...")
    if "Greedy" in algorithms:
        engine.register_ai("Greedy", GreedyAgent(distance=2))
    if "Minimax-D2" in algorithms:
        engine.register_ai("Minimax-D2", MinimaxAgent(depth=2, distance=2, candidate_limit=10))
    if "AlphaBeta-D2" in algorithms:
        engine.register_ai("AlphaBeta-D2", AlphaBetaAgent(depth=2, distance=2, candidate_limit=10))
    if "MCTS-300" in algorithms:
        engine.register_ai("MCTS-300", MCTSAgent(iteration_limit=300))
    if "DQN" in algorithms:
        try:
            dqn_agent = QLearningAgent(model_path="models/dqn_15x15_final")
            engine.register_ai("DQN", dqn_agent)
        except Exception as e:
            print(f"   âš ï¸ DQN not available: {e}")
    
    print(f"\nâœ“ æ³¨å†Œäº† {len(engine.ai_algorithms)} ä¸ªAI")
    
    # è¿è¡Œå¾ªç¯èµ›
    print(f"\nå¼€å§‹æ‰¹æ¬¡ {batch_id} çš„å¯¹å±€...")
    results = engine.run_round_robin(
        num_games_per_pair=games_per_batch, 
        verbose=True,
        resume=False
    )
    
    # ä¿å­˜æ‰¹æ¬¡ç»“æœ
    output_path = get_batch_output_path(batch_id)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    batch_data = {
        'batch_id': batch_id,
        'total_batches': total_batches,
        'games_per_batch': games_per_batch,
        'timestamp': datetime.now().isoformat(),
        'algorithms': list(engine.ai_algorithms.keys()),
        'results': [r.to_dict() for r in results]
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(batch_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… æ‰¹æ¬¡ {batch_id} å®Œæˆ!")
    print(f"   å…± {len(results)} å±€æ¸¸æˆ")
    print(f"   ç»“æœä¿å­˜åˆ°: {output_path}")
    
    engine.cleanup()
    return results


def merge_batches():
    """åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ"""
    print("=" * 70)
    print(" åˆå¹¶æ‰¹æ¬¡ç»“æœ")
    print("=" * 70)
    
    # æŸ¥æ‰¾æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶
    batch_files = sorted(Path("./data/results/self_play").glob("batch_*.json"))
    
    if not batch_files:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ‰¹æ¬¡æ–‡ä»¶!")
        print("   è¯·å…ˆè¿è¡Œæ‰¹æ¬¡: python scripts/parallel_eval.py --batch-id 1")
        return
    
    print(f"\næ‰¾åˆ° {len(batch_files)} ä¸ªæ‰¹æ¬¡æ–‡ä»¶:")
    for bf in batch_files:
        print(f"  - {bf.name}")
    
    # åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰ç»“æœ
    all_results = []
    batch_info = []
    
    for batch_file in batch_files:
        with open(batch_file, 'r', encoding='utf-8') as f:
            batch_data = json.load(f)
        
        batch_info.append({
            'batch_id': batch_data['batch_id'],
            'games': len(batch_data['results']),
            'timestamp': batch_data['timestamp']
        })
        
        all_results.extend(batch_data['results'])
    
    print(f"\nåˆå¹¶ç»Ÿè®¡:")
    print(f"  æ€»æ‰¹æ¬¡æ•°: {len(batch_files)}")
    print(f"  æ€»æ¸¸æˆæ•°: {len(all_results)}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = get_merge_output_dir()
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜åˆå¹¶åçš„è¯¦ç»†ç»“æœ
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    merged_json = output_dir / f"merged_results_{timestamp}.json"
    
    merged_data = {
        'merge_timestamp': datetime.now().isoformat(),
        'total_games': len(all_results),
        'batches': batch_info,
        'results': all_results
    }
    
    with open(merged_json, 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ“ è¯¦ç»†ç»“æœ: {merged_json}")
    
    # ä¿å­˜CSVæ ¼å¼
    df = pd.DataFrame(all_results)
    csv_path = output_dir / f"merged_results_{timestamp}.csv"
    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    
    print(f"âœ“ CSVç»“æœ: {csv_path}")
    
    # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
    print(f"\nğŸ“Š æ±‡æ€»ç»Ÿè®¡:")
    print(f"   å¹³å‡æ­¥æ•°: {df['total_moves'].mean():.1f}")
    print(f"   å¹³å‡æ—¶é—´: {(df['player1_avg_time'] + df['player2_avg_time']).mean() / 2:.3f}s")
    
    # èƒœç‡ç»Ÿè®¡
    print(f"\nğŸ† èƒœç‡ç»Ÿè®¡:")
    ai_names = set(df['player1'].unique()) | set(df['player2'].unique())
    for ai in sorted(ai_names):
        df_as_p1 = df[df['player1'] == ai]
        df_as_p2 = df[df['player2'] == ai]
        
        p1_wins = len(df_as_p1[df_as_p1['winner'] == 'player1'])
        p2_wins = len(df_as_p2[df_as_p2['winner'] == 'player2'])
        
        total_games = len(df_as_p1) + len(df_as_p2)
        total_wins = p1_wins + p2_wins
        
        if total_games > 0:
            win_rate = total_wins / total_games * 100
            print(f"   {ai:20s}: {total_wins:3d}/{total_games:3d} = {win_rate:5.1f}%")
    
    print("\n" + "=" * 70)
    print(" åˆå¹¶å®Œæˆ!")
    print("=" * 70)
    print(f"\nğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")


def main():
    parser = argparse.ArgumentParser(description="åˆ†å¸ƒå¼è‡ªå¯¹å¼ˆè¯„ä¼°ç³»ç»Ÿ")
    
    # æ‰¹æ¬¡æ¨¡å¼
    parser.add_argument('--batch-id', type=int, help='æ‰¹æ¬¡ID (1-based)')
    parser.add_argument('--total-batches', type=int, default=4, help='æ€»æ‰¹æ¬¡æ•°')
    parser.add_argument('--games-per-batch', type=int, default=5, 
                       help='æ¯ä¸ªé…å¯¹åœ¨æ¯ä¸ªæ‰¹æ¬¡ä¸­çš„æ¸¸æˆæ•°')
    
    # åˆå¹¶æ¨¡å¼
    parser.add_argument('--merge', action='store_true', help='åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡ç»“æœ')
    
    # ç®—æ³•é€‰æ‹©
    parser.add_argument('--algorithms', nargs='+', 
                       default=["Greedy", "Minimax-D2", "AlphaBeta-D2", "MCTS-300", "DQN"],
                       help='è¦æµ‹è¯•çš„ç®—æ³•')
    
    args = parser.parse_args()
    
    if args.merge:
        # åˆå¹¶æ¨¡å¼
        merge_batches()
    elif args.batch_id:
        # æ‰¹æ¬¡æ¨¡å¼
        run_batch(args.batch_id, args.total_batches, args.games_per_batch, args.algorithms)
    else:
        # æ˜¾ç¤ºå¸®åŠ©
        parser.print_help()
        print("\nç¤ºä¾‹ç”¨æ³•:")
        print("  # è¿è¡Œæ‰¹æ¬¡1 (å…±4ä¸ªæ‰¹æ¬¡)")
        print("  python scripts/parallel_eval.py --batch-id 1 --total-batches 4 --games-per-batch 5")
        print("\n  # è¿è¡Œæ‰¹æ¬¡2")
        print("  python scripts/parallel_eval.py --batch-id 2 --total-batches 4 --games-per-batch 5")
        print("\n  # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡")
        print("  python scripts/parallel_eval.py --merge")


if __name__ == "__main__":
    main()
