"""æ€§èƒ½åˆ†æè„šæœ¬ (Person E/G)

æ•°æ®é¢„å¤„ç† + ç»Ÿè®¡åˆ†æ + æ€§èƒ½è¯„ä¼°
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
from glob import glob
from pathlib import Path

from backend.services.performance_analyzer import StatisticalAnalyzer


# ==================== æ•°æ®é¢„å¤„ç†æ¨¡å— ====================

def preprocess_data(data_path: str) -> pd.DataFrame:
    """æ•°æ®é¢„å¤„ç†ï¼šæ¸…æ´—ã€æ ‡å‡†åŒ–ã€éªŒè¯
    
    Args:
        data_path: åŸå§‹æ•°æ®CSVè·¯å¾„
        
    Returns:
        é¢„å¤„ç†åçš„DataFrame
    """
    print(f"\nğŸ“‚ Loading data from: {data_path}")
    df = pd.read_csv(data_path)
    original_count = len(df)
    
    # 1. æ•°æ®æ¸…æ´—
    # ç§»é™¤å¼‚å¸¸å€¼ï¼ˆè¶…é•¿å¯¹å±€ï¼‰
    q99 = df['total_moves'].quantile(0.99)
    df = df[df['total_moves'] <= q99]
    
    # ç§»é™¤å¼‚å¸¸è€—æ—¶ï¼ˆè¶…è¿‡60ç§’ï¼‰
    df = df[df['player1_avg_time'] < 60]
    df = df[df['player2_avg_time'] < 60]
    
    # ç§»é™¤ç¼ºå¤±å€¼
    df = df.dropna()
    
    # ç¡®ä¿winnerå­—æ®µåˆæ³•
    valid_winners = ['player1', 'player2', 'draw']
    df = df[df['winner'].isin(valid_winners)]
    
    cleaned_count = len(df)
    removed = original_count - cleaned_count
    print(f"âœ“ Cleaned: {original_count} -> {cleaned_count} records (removed {removed})")
    
    # 2. ç‰¹å¾å·¥ç¨‹
    df['total_time'] = df['player1_avg_time'] + df['player2_avg_time']
    df['time_difference'] = np.abs(df['player1_avg_time'] - df['player2_avg_time'])
    df['faster_player'] = np.where(
        df['player1_avg_time'] < df['player2_avg_time'],
        'player1',
        'player2'
    )
    
    # ç¼–ç èƒœè€…
    df['player1_won'] = (df['winner'] == 'player1').astype(int)
    df['player2_won'] = (df['winner'] == 'player2').astype(int)
    df['is_draw'] = (df['winner'] == 'draw').astype(int)
    
    # å¯¹å±€é•¿åº¦åˆ†ç±»
    df['game_length_category'] = pd.cut(
        df['total_moves'],
        bins=[0, 20, 40, 60, np.inf],
        labels=['short', 'medium', 'long', 'very_long']
    )
    
    print(f"âœ“ Added {len(df.columns) - len(pd.read_csv(data_path).columns)} derived features")
    
    # 3. æ•°æ®éªŒè¯
    print(f"\nğŸ“Š Data Validation:")
    print(f"   Total records: {len(df)}")
    unique_algos = len(set(df['player1'].unique()) | set(df['player2'].unique()))
    print(f"   Unique algorithms: {unique_algos}")
    print(f"   Winner distribution: {df['winner'].value_counts().to_dict()}")
    print(f"   Avg moves: {df['total_moves'].mean():.1f} Â± {df['total_moves'].std():.1f}")
    print(f"   Missing values: {df.isnull().sum().sum()}")
    
    return df


def save_preprocessed_data(df: pd.DataFrame, output_path: str):
    """ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®"""
    Path(output_path).parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"\nâœ“ Saved preprocessed data to {output_path}")


# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»ç¨‹åº"""
    print("=" * 60)
    print(" Performance Analysis Pipeline")
    print("=" * 60)
    
    # 1. æ‰¾åˆ°æœ€æ–°çš„æ•°æ®æ–‡ä»¶
    data_files = glob("./data/results/self_play/aggregated/results_*.csv")
    
    if not data_files:
        print("\nâŒ No data files found!")
        print("   Please run scripts/eval_models.py first to generate data.")
        return
    
    latest_file = max(data_files, key=os.path.getctime)
    
    # 2. æ•°æ®é¢„å¤„ç†
    print("\n" + "=" * 60)
    print(" STEP 1: Data Preprocessing")
    print("=" * 60)
    
    preprocessed_data = preprocess_data(latest_file)
    
    # ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®
    output_path = "./data/results/self_play/preprocessed_data.csv"
    save_preprocessed_data(preprocessed_data, output_path)
    
    # æ‰“å°æ±‡æ€»ç»Ÿè®¡
    print("\nğŸ“Š Summary Statistics:")
    numeric_cols = ['total_moves', 'player1_avg_time', 'player2_avg_time', 'total_time']
    summary = preprocessed_data[numeric_cols].describe()
    print(summary)
    
    # 3. ç»Ÿè®¡åˆ†æ
    print("\n" + "=" * 60)
    print(" STEP 2: Statistical Analysis")
    print("=" * 60)
    
    analyzer = StatisticalAnalyzer(preprocessed_data)
    
    # 3.1 èƒœç‡ç»Ÿè®¡
    win_rates = analyzer.calculate_win_rates()
    win_rates.to_csv("./data/results/win_rates.csv", index=False)
    print(f"\nâœ“ Saved win rates to ./data/results/win_rates.csv")
    
    # 3.2 å“åº”æ—¶é—´ç»Ÿè®¡
    time_stats = analyzer.calculate_response_times()
    time_stats.to_csv("./data/results/response_times.csv", index=False)
    print(f"âœ“ Saved response times to ./data/results/response_times.csv")
    
    # 3.3 å¯¹æˆ˜çŸ©é˜µ
    matchup_matrix = analyzer.generate_matchup_matrix()
    matchup_matrix.to_csv("./data/results/matchup_matrix.csv")
    print(f"âœ“ Saved matchup matrix to ./data/results/matchup_matrix.csv")
    
    # 3.4 æ˜¾è‘—æ€§æ£€éªŒ
    significance_tests = analyzer.run_all_pairwise_tests()
    significance_tests.to_csv("./data/results/significance_tests.csv", index=False)
    print(f"âœ“ Saved significance tests to ./data/results/significance_tests.csv")
    
    # 3.5 ELOè¯„åˆ†
    elo_ratings = analyzer.calculate_elo_ratings(k_factor=32.0)
    elo_ratings.to_csv("./data/results/elo_ratings.csv", index=False)
    print(f"âœ“ Saved ELO ratings to ./data/results/elo_ratings.csv")
    
    # 4. æ€»ç»“
    print("\n" + "=" * 60)
    print(" Analysis Complete!")
    print("=" * 60)
    print("\nâœ… All analysis results saved to ./data/results/")
    print("\nGenerated files:")
    print("  - preprocessed_data.csv      : Cleaned and standardized data")
    print("  - win_rates.csv              : Win rate statistics")
    print("  - response_times.csv         : Response time statistics")
    print("  - matchup_matrix.csv         : Head-to-head win rates")
    print("  - significance_tests.csv     : Statistical significance tests")
    print("  - elo_ratings.csv            : ELO ratings")
    
    print("\nğŸ¯ Next step: Run scripts/generate_visualizations.py to create charts")


if __name__ == "__main__":
    main()
